
---

## üñ• Atividades Individuais

**Air Quality Dataset:**
https://github.com/klaytoncastro/idp-machinelearning/tree/main/airquality

**Bank Dataset:**
https://github.com/klaytoncastro/idp-machinelearning/tree/main/bank

**California Dataset:**
https://github.com/klaytoncastro/idp-machinelearning/tree/main/california

**Mall Customers Dataset:**
https://github.com/klaytoncastro/idp-machinelearning/tree/main/clustering

**Iris Dataset:**
https://github.com/klaytoncastro/idp-machinelearning/tree/main/iris

---

## üå™Ô∏è Air Quality Dataset

<details>

# An√°lise do Air Quality Dataset

Este reposit√≥rio cont√©m um notebook em Python com a an√°lise explorat√≥ria de dados e aplica√ß√£o de t√©cnicas de Machine Learning no **Air Quality Dataset**, como parte das atividades pr√°ticas de aprendizado.

## Estrutura do Reposit√≥rio

- **`Airquality_Felipe_Barroso.ipynb`**: Notebook principal contendo todas as etapas do projeto.
- **`AirQualityUCI.csv`**: Arquivo com os dados originais utilizados no projeto.

## Objetivo

O objetivo principal deste projeto foi analisar a qualidade do ar com base em medi√ß√µes de gases e vari√°veis clim√°ticas, al√©m de criar modelos preditivos para estimar concentra√ß√µes de CO no ar.

## Etapas Realizadas

1. **Carregamento e Limpeza de Dados**
   - Importa√ß√£o do dataset em formato CSV.
   - Tratamento de valores ausentes preenchendo com a mediana.
   - Remo√ß√£o de colunas irrelevantes ou n√£o num√©ricas.

2. **An√°lise Explorat√≥ria de Dados (EDA)**
   - Gera√ß√£o de estat√≠sticas descritivas.
   - Visualiza√ß√µes gr√°ficas das distribui√ß√µes das vari√°veis.
   - Matriz de correla√ß√£o para identificar rela√ß√µes entre vari√°veis.

3. **Pr√©-processamento**
   - Sele√ß√£o de vari√°veis independentes (features) e dependente (target).
   - Divis√£o do dataset em conjuntos de treino e teste.

4. **Modelagem de Machine Learning**
   - Treinamento de um modelo de Random Forest para prever concentra√ß√µes de CO.
   - Avalia√ß√£o do modelo com m√©tricas:
     - Mean Squared Error (MSE)
     - R-squared (R¬≤)

5. **Resultados**
   - Compara√ß√£o entre os valores reais e previstos para avaliar a efic√°cia do modelo.

6. **Conclus√µes**
   - O modelo conseguiu capturar padr√µes relevantes no dataset, apresentando um desempenho satisfat√≥rio para predi√ß√£o de CO.

## Ferramentas Utilizadas

- **Linguagem**: Python
- **Bibliotecas**:
  - `pandas`
  - `numpy`
  - `matplotlib`
  - `seaborn`
  - `scikit-learn`

## Observa√ß√µes

- Certifique-se de que o arquivo `AirQualityUCI.csv` esteja no mesmo diret√≥rio do notebook ao execut√°-lo.

</details>

---

## üè¶ Bank Dataset

<details>

# An√°lise do Bank Dataset

Este reposit√≥rio cont√©m um notebook em Python com a an√°lise explorat√≥ria de dados e aplica√ß√£o de t√©cnicas de Machine Learning no **Bank Dataset**, parte das atividades pr√°ticas do curso de Machine Learning.

## Estrutura do Reposit√≥rio

- **`Bank_Dataset_Felipe_Barroso.ipynb`**: Notebook principal contendo todas as etapas do projeto.
- **`Bank_Data.csv`**: Arquivo com os dados originais utilizados no projeto.

## Objetivo

O objetivo principal deste projeto foi analisar os dados de uma campanha de marketing de uma institui√ß√£o banc√°ria e criar modelos preditivos para estimar a probabilidade de um cliente aceitar um produto banc√°rio (como um dep√≥sito a prazo fixo).

## Etapas Realizadas

1. **Carregamento dos Dados**
   - Importa√ß√£o do arquivo `Bank_Data.csv` para o ambiente de trabalho.
   - Inspe√ß√£o inicial dos dados para compreender suas vari√°veis e estrutura.

2. **An√°lise Explorat√≥ria de Dados (EDA)**
   - Estat√≠sticas descritivas das vari√°veis.
   - Visualiza√ß√µes gr√°ficas para identificar padr√µes e rela√ß√µes entre os dados.
   - Tratamento de dados ausentes e outliers.

3. **Pr√©-processamento**
   - Convers√£o de vari√°veis categ√≥ricas para representa√ß√µes num√©ricas.
   - Normaliza√ß√£o de vari√°veis para uso em modelos de aprendizado de m√°quina.
   - Divis√£o dos dados em conjuntos de treinamento e teste.

4. **Modelagem**
   - Treinamento de diferentes algoritmos de classifica√ß√£o:
     - Regress√£o Log√≠stica
     - √Årvore de Decis√£o
     - Random Forest
   - Avalia√ß√£o de desempenho dos modelos utilizando m√©tricas como:
     - Acur√°cia
     - Precis√£o
     - Recall
     - F1-score

5. **Resultados**
   - Compara√ß√£o entre os modelos com base nas m√©tricas de avalia√ß√£o.
   - Identifica√ß√£o do melhor modelo para o problema proposto.

6. **Conclus√µes**
   - Insights gerados a partir dos dados.
   - Discuss√£o sobre a aplicabilidade do modelo no mundo real.

## Ferramentas Utilizadas

- Python (bibliotecas: pandas, numpy, matplotlib, seaborn, scikit-learn)
- Jupyter Notebook

## Observa√ß√µes

- Certifique-se de que o arquivo `Bank_Data.csv` esteja no mesmo diret√≥rio do notebook.

</details>

---

## ‚õ±Ô∏è California Dataset

<details>

# An√°lise do California Housing Dataset

Este reposit√≥rio cont√©m um notebook em Python com a an√°lise explorat√≥ria, pr√©-processamento e aplica√ß√£o de modelos de machine learning no **California Housing Dataset**.

## Estrutura do Reposit√≥rio

- **`california_housing_analysis.ipynb`**: Notebook contendo a an√°lise completa e modelagem dos dados.
- **`california_housing.csv`**: Arquivo com os dados originais utilizados no projeto.

## Objetivo

O objetivo principal deste projeto foi prever o valor m√©dio de casas em diferentes regi√µes da Calif√≥rnia, utilizando vari√°veis como idade m√©dia das casas, n√∫mero m√©dio de quartos, ocupa√ß√£o m√©dia, latitude, longitude, entre outras.

## Etapas do Projeto

### 1. An√°lise Explorat√≥ria de Dados (EDA)
- **Distribui√ß√£o das Vari√°veis**: Analisamos as distribui√ß√µes das vari√°veis para identificar outliers e entender caracter√≠sticas gerais do conjunto de dados.
- **Correla√ß√£o**: Exploramos as correla√ß√µes entre as vari√°veis preditoras e a vari√°vel alvo (`MedHouseVal`).
- **Gr√°ficos**:
  - Histogramas para distribui√ß√£o de vari√°veis.
  - Matriz de correla√ß√£o para identificar rela√ß√µes entre as vari√°veis.

### 2. Pr√©-processamento de Dados
- **Tratamento de Valores Ausentes**: Identifica√ß√£o e tratamento de valores ausentes (se aplic√°vel).
- **Normaliza√ß√£o**: Vari√°veis foram normalizadas com `StandardScaler` para garantir que todas as vari√°veis estivessem na mesma escala.
- **Divis√£o dos Dados**: O dataset foi dividido em conjuntos de treino (80%) e teste (20%) para valida√ß√£o dos modelos.

### 3. Modelagem
Foram aplicados e avaliados diferentes algoritmos de regress√£o para prever o valor m√©dio das casas:
- **Regress√£o Linear**: Modelo b√°sico para estabelecer uma linha de base.
- **√Årvore de Decis√£o**: Modelo n√£o linear para capturar intera√ß√µes complexas.
- **Random Forest**: Modelo de ensemble para melhorar a performance e reduzir overfitting.
- **SVR (Support Vector Regressor)**: Modelo de regress√£o baseado no algoritmo SVM.
- **Gradient Boosting Machines**: Implementa√ß√£o com `GradientBoostingRegressor`.

### 4. Avalia√ß√£o e Interpreta√ß√£o
- **M√©tricas de Avalia√ß√£o**:
  - **MAE (Mean Absolute Error)**: Erro m√©dio absoluto.
  - **MSE (Mean Squared Error)**: Erro quadr√°tico m√©dio.
  - **R¬≤ (Coeficiente de Determina√ß√£o)**: Mede a propor√ß√£o da vari√¢ncia explicada pelo modelo.
- **Gr√°ficos de Valores Reais x Preditos**:
  - Para cada modelo, geramos gr√°ficos de dispers√£o comparando os valores reais e os previstos.

## Ferramentas Utilizadas

- **Linguagem**: Python
- **Bibliotecas**:
  - `pandas`
  - `numpy`
  - `matplotlib`
  - `seaborn`
  - `scikit-learn`

## Resultados

- **Modelos Avaliados**:
  - Regress√£o Linear apresentou bom desempenho como modelo inicial.
  - Random Forest e Gradient Boosting Machines se destacaram pelo melhor equil√≠brio entre bias e vari√¢ncia.
- **Insights**:
  - Vari√°veis como `MedInc` (Renda M√©dia) mostraram forte correla√ß√£o com o valor m√©dio das casas.
  - Modelos ensemble como Random Forest e Gradient Boosting s√£o mais adequados para capturar padr√µes complexos.

</details>

---

## üè¢ Mall Customers Dataset

<details>

# An√°lise do Mall Customers Dataset

Este reposit√≥rio cont√©m um notebook em Python com a an√°lise explorat√≥ria, pr√©-processamento e aplica√ß√£o de algoritmos de clustering no **Mall Customers Dataset**.

## Estrutura do Reposit√≥rio

- **`mall_customers_analysis.ipynb`**: Notebook contendo a an√°lise completa e modelagem dos dados.
- **`mall_customers.csv`**: Arquivo com os dados originais utilizados no projeto.

## Objetivo

O objetivo principal deste projeto foi agrupar clientes em clusters com base em suas caracter√≠sticas de renda anual e pontua√ß√£o de gastos, utilizando t√©cnicas de clustering.

## Etapas do Projeto

### 1. An√°lise Explorat√≥ria de Dados (EDA)
- **Distribui√ß√£o das Vari√°veis**:
  - Foram analisadas distribui√ß√µes das vari√°veis `Age`, `Annual Income (k$)` e `Spending Score (1-100)` para identificar padr√µes e poss√≠veis outliers.
- **Gr√°ficos**:
  - Histogramas foram criados para visualizar a distribui√ß√£o de cada vari√°vel.

### 2. Pr√©-processamento
- **Sele√ß√£o de Vari√°veis**: As vari√°veis `Annual Income (k$)` e `Spending Score (1-100)` foram escolhidas para a modelagem de clustering.
- **Normaliza√ß√£o**: Os dados foram normalizados usando `StandardScaler` para garantir que as vari√°veis estivessem na mesma escala.

### 3. Modelagem com Clustering
- **K-Means**:
  - O m√©todo Elbow foi utilizado para determinar o n√∫mero ideal de clusters (5 clusters foram selecionados).
  - Clientes foram agrupados em clusters com base em sua similaridade.
  - Avalia√ß√£o do modelo foi feita com o Silhouette Score.
- **DBSCAN (opcional)**:
  - Foi aplicada uma an√°lise complementar com DBSCAN para identificar clusters baseados em densidade.

### 4. Visualiza√ß√£o dos Clusters
- Gr√°ficos de dispers√£o foram gerados para visualizar os clusters com base nas vari√°veis `Annual Income (k$)` e `Spending Score (1-100)`.

## Ferramentas Utilizadas

- **Linguagem**: Python
- **Bibliotecas**:
  - `pandas`
  - `numpy`
  - `matplotlib`
  - `seaborn`
  - `sklearn`

## Resultados

- **Clusters Identificados**:
  - K-Means agrupou clientes em 5 clusters distintos, com base em renda anual e comportamento de gastos.
  - DBSCAN identificou clusters alternativos baseados em densidade (opcional).
- **Insights**:
  - Clientes com renda alta e alta pontua√ß√£o de gastos foram agrupados em clusters espec√≠ficos, destacando consumidores de alto valor.
  - O Silhouette Score confirmou a qualidade dos clusters gerados pelo K-Means.

</details>

---

## üëÅÔ∏è Iris Dataset

<details>

# An√°lise do Iris Dataset

Este reposit√≥rio cont√©m um notebook em Python com a an√°lise explorat√≥ria, pr√©-processamento e aplica√ß√£o de algoritmos de classifica√ß√£o no **Iris Dataset**.

## Estrutura do Reposit√≥rio

- **`iris_analysis.ipynb`**: Notebook contendo a an√°lise completa e modelagem dos dados.
- **`iris.csv`**: Arquivo com os dados originais utilizados no projeto.

## Objetivo

O objetivo principal deste projeto foi classificar as esp√©cies de flores (Setosa, Versicolor e Virginica) com base em caracter√≠sticas como comprimento e largura de s√©palas e p√©talas.

## Etapas do Projeto

### 1. An√°lise Explorat√≥ria de Dados (EDA)
- **Estat√≠sticas Descritivas**: Resumo das distribui√ß√µes das vari√°veis.
- **Rela√ß√µes entre Vari√°veis**:
  - Gr√°ficos de dispers√£o para identificar padr√µes nas vari√°veis.
  - Matriz de correla√ß√£o para avaliar rela√ß√µes entre vari√°veis num√©ricas.

### 2. Pr√©-processamento
- **Separa√ß√£o de Vari√°veis**: As vari√°veis preditoras (`sepal_length`, `sepal_width`, `petal_length`, `petal_width`) foram separadas da vari√°vel alvo (`species`).
- **Normaliza√ß√£o**: As vari√°veis preditoras foram normalizadas para melhorar o desempenho dos modelos.
- **Divis√£o dos Dados**: O dataset foi dividido em conjuntos de treino (80%) e teste (20%) para valida√ß√£o dos modelos.

### 3. Modelagem com Algoritmos de Classifica√ß√£o
Foram aplicados e avaliados os seguintes algoritmos de classifica√ß√£o:
- **K-Nearest Neighbors (KNN)**:
  - Classifica os dados com base nos vizinhos mais pr√≥ximos.
- **Decision Tree**:
  - Modelo baseado em √°rvores de decis√£o para regras interpret√°veis.
- **Random Forest**:
  - Combina m√∫ltiplas √°rvores para maior precis√£o e redu√ß√£o de overfitting.
- **Support Vector Machine (SVM)**:
  - Separa classes com hiperplanos em espa√ßos de alta dimens√£o.

### 4. Avalia√ß√£o do Modelo
- **M√©tricas Utilizadas**:
  - Matriz de Confus√£o: Mostra os acertos e erros de classifica√ß√£o para cada classe.
  - Relat√≥rio de Classifica√ß√£o: Fornece precis√£o, recall e F1-Score.
  - Acur√°cia Geral: Mede o percentual de predi√ß√µes corretas.
- **Resultados**:
  - Todos os modelos apresentaram alto desempenho devido √† simplicidade do dataset Iris.
  - O modelo Random Forest foi o mais robusto, com a melhor combina√ß√£o de precis√£o e recall.

## Ferramentas Utilizadas

- **Linguagem**: Python
- **Bibliotecas**:
  - `pandas`
  - `numpy`
  - `matplotlib`
  - `seaborn`
  - `sklearn`

## Resultados

- **Insights**:
  - As vari√°veis `petal_length` e `petal_width` t√™m maior correla√ß√£o com a classifica√ß√£o das esp√©cies.
  - Modelos como Random Forest e SVM s√£o ideais para lidar com problemas de classifica√ß√£o em datasets simples como o Iris.

</details>

---

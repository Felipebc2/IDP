{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9265,"status":"ok","timestamp":1733437551980,"user":{"displayName":"Master fbc2","userId":"17579304159483715433"},"user_tz":180},"id":"GKG9TfUAXQ6G","outputId":"5141876c-ffaa-4a80-9db3-597f9a86c512"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ucimlrepo\n","  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\n","Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2.2.2)\n","Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2024.8.30)\n","Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.16.0)\n","Downloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\n","Installing collected packages: ucimlrepo\n","Successfully installed ucimlrepo-0.0.7\n"]}],"source":["pip install ucimlrepo"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":11172,"status":"ok","timestamp":1733437563145,"user":{"displayName":"Master fbc2","userId":"17579304159483715433"},"user_tz":180},"id":"yJs07pFyT65a"},"outputs":[],"source":["# Importando as bibliotecas necessárias\n","from tensorflow.keras.layers import Input, Dense\n","from tensorflow.keras.models import Model\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n","from tensorflow import keras\n","from tensorflow.keras.layers import Input, Dense\n","from tensorflow.keras.models import Model\n","import numpy as np"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4101,"status":"ok","timestamp":1733437567233,"user":{"displayName":"Master fbc2","userId":"17579304159483715433"},"user_tz":180},"id":"j-ZbFEscXIBY","outputId":"bc09da3e-bea9-4f43-a8cc-8de2165a55ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'uci_id': 352, 'name': 'Online Retail', 'repository_url': 'https://archive.ics.uci.edu/dataset/352/online+retail', 'data_url': 'https://archive.ics.uci.edu/static/public/352/data.csv', 'abstract': 'This is a transactional data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.', 'area': 'Business', 'tasks': ['Classification', 'Clustering'], 'characteristics': ['Multivariate', 'Sequential', 'Time-Series'], 'num_instances': 541909, 'num_features': 6, 'feature_types': ['Integer', 'Real'], 'demographics': [], 'target_col': None, 'index_col': ['InvoiceNo', 'StockCode'], 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 2015, 'last_updated': 'Mon Oct 21 2024', 'dataset_doi': '10.24432/C5BW33', 'creators': ['Daqing Chen'], 'intro_paper': {'ID': 361, 'type': 'NATIVE', 'title': 'Data mining for the online retail industry: A case study of RFM model-based customer segmentation using data mining', 'authors': 'Daqing Chen, Sai Laing Sain, Kun Guo', 'venue': 'Journal of Database Marketing and Customer Strategy Management, Vol. 19, No. 3', 'year': 2012, 'journal': None, 'DOI': '10.1057/dbm.2012.17', 'URL': 'https://www.semanticscholar.org/paper/e43a5a90fa33d419df42e485099f8f08badf2149', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': 'This is a transactional data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': \"InvoiceNo: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation. \\nStockCode: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.\\nDescription: Product (item) name. Nominal.\\nQuantity: The quantities of each product (item) per transaction. Numeric.\\t\\nInvoiceDate: Invoice Date and time. Numeric, the day and time when each transaction was generated.\\nUnitPrice: Unit price. Numeric, Product price per unit in sterling.\\nCustomerID: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.\\nCountry: Country name. Nominal, the name of the country where each customer resides. \", 'citation': None}}\n","          name     role         type demographic  \\\n","0    InvoiceNo       ID  Categorical        None   \n","1    StockCode       ID  Categorical        None   \n","2  Description  Feature  Categorical        None   \n","3     Quantity  Feature      Integer        None   \n","4  InvoiceDate  Feature         Date        None   \n","5    UnitPrice  Feature   Continuous        None   \n","6   CustomerID  Feature  Categorical        None   \n","7      Country  Feature  Categorical        None   \n","\n","                                         description     units missing_values  \n","0  a 6-digit integral number uniquely assigned to...      None             no  \n","1  a 5-digit integral number uniquely assigned to...      None             no  \n","2                                       product name      None             no  \n","3  the quantities of each product (item) per tran...      None             no  \n","4  the day and time when each transaction was gen...      None             no  \n","5                             product price per unit  sterling             no  \n","6  a 5-digit integral number uniquely assigned to...      None             no  \n","7  the name of the country where each customer re...      None             no  \n"]}],"source":["from ucimlrepo import fetch_ucirepo\n","\n","# fetch dataset\n","online_retail = fetch_ucirepo(id=352)\n","\n","# data (as pandas dataframes)\n","X = online_retail.data.features\n","y = online_retail.data.targets\n","\n","# metadata\n","print(online_retail.metadata)\n","\n","# variable information\n","print(online_retail.variables)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1733437567233,"user":{"displayName":"Master fbc2","userId":"17579304159483715433"},"user_tz":180},"id":"Z-YoFT_qEvxm"},"outputs":[],"source":["# Retirando a coluna de Data e Hora da compra\n","X = X.drop(columns=['InvoiceDate'])"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1733437567233,"user":{"displayName":"Master fbc2","userId":"17579304159483715433"},"user_tz":180},"id":"9Uo0iZuZS9Kg"},"outputs":[],"source":["# Channel Islands não possui DDI próprio então foi colocado o número 4\n","# Canadá possui o mesmo DDI que USA então foi alterado pra 11\n","# European Community não possui DDI próprio então foi colocado o número 3\n","\n","country_mapping = {\n","    'United Kingdom': 44,\n","    'Germany': 49,\n","    'France': 33,\n","    'EIRE': 353,\n","    'Spain': 34,\n","    'Netherlands': 31,\n","    'Belgium': 32,\n","    'Switzerland': 41,\n","    'Portugal': 351,\n","    'Australia': 61,\n","    'Norway': 47,\n","    'Italy': 39,\n","    'Channel Islands': 4,\n","    'Finland': 358,\n","    'Cyprus': 657,\n","    'Sweden': 46,\n","    'Unspecified': 0,\n","    'Austria': 43,\n","    'Denmark': 45,\n","    'Japan': 81,\n","    'Poland': 48,\n","    'Israel': 972,\n","    'USA': 1,\n","    'Hong Kong': 852,\n","    'Singapore': 65,\n","    'Iceland': 354,\n","    'Canada': 11,\n","    'Greece': 30,\n","    'Malta': 356,\n","    'United Arab Emirates': 971,\n","    'European Community': 3,\n","    'RSA': 27,\n","    'Lebanon': 961,\n","    'Lithuania': 370,\n","    'Brazil': 55,\n","    'Czech Republic': 420,\n","    'Bahrain': 973,\n","    'Saudi Arabia': 966\n","}\n","\n","X['Country'] = X['Country'].map(country_mapping).astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"INGD-lV1d9QO"},"outputs":[],"source":["X_sample = X.sample(frac=0.5, random_state=42)  # Seleciona 50% dos dados\n","X['Quantity'] = X['Quantity'].astype('int32')\n","X['Country'] = X['Country'].astype('int16')\n","\n","\n","# Usar pd.get_dummies para transformar Description em colunas de 0 e 1\n","df_encoded = pd.get_dummies(X_sample, columns=['Description'], prefix='', prefix_sep='').fillna(0).astype(int)\n","\n","# Ver resultado\n","print(df_encoded.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aWc5sBu3OwK9"},"outputs":[],"source":["print(X_sample['Country'].value_counts())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B3fyiONiN0vC"},"outputs":[],"source":["X_sample.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PKOgRyOFbuOq"},"outputs":[],"source":["print(X_sample.describe())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kYGvPc6mc0Zw"},"outputs":[],"source":["numerical_cols = X_sample.select_dtypes(include=['number']).columns\n","scaler = MinMaxScaler()\n","X_sample[numerical_cols] = scaler.fit_transform(X_sample[numerical_cols])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RkaXMswyc5eD"},"outputs":[],"source":["string_cols = X_sample.select_dtypes(include=['object']).columns\n","numerical_cols = X.select_dtypes(include=['number']).columns\n","X_numerical = X[numerical_cols]\n","print(X.isnull().sum())  # Check for NaN values\n","\n","\n","label_encoders = {}\n","for col in string_cols:\n","    label_encoders[col] = LabelEncoder()\n","    X_sample[col] = label_encoders[col].fit_transform(X_sample[col])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xsrsCi9JU4ZR"},"outputs":[],"source":["input_dim = X_sample.shape[1]  # 4 itens de entrada no dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KoOoychhU7nu"},"outputs":[],"source":["from tensorflow import keras\n","# Construir o Autoencoder\n","input_layer = Input(shape=(input_dim,))\n","encoder = Dense(3, activation=\"relu\")(input_layer)  # Consider using 'tanh' or 'linear'\n","decoder = Dense(input_dim, activation=\"linear\")(encoder) # Use 'linear' for regression\n","autoencoder = Model(inputs=input_layer, outputs=decoder)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j8QPqI5hVDrd"},"outputs":[],"source":["# Definir o modelo\n","autoencoder.compile(optimizer='adam', loss='mse')  # Use 'mse' for regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HHWYK9emVOmS"},"outputs":[],"source":["# # Compilar o modelo\n","# autoencoder.fit(X_sample, X_sample, epochs=100, verbose=0)\n","\n","autoencoder.fit(X_sample, X_sample, epochs=30, batch_size=64, verbose=0)\n","# Utilizando a Sample"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IV5pb36wVSTf"},"outputs":[],"source":["# Fazer previsões (reconstrução das entradas)\n","reconstructed = autoencoder.predict(X_sample)\n","print(reconstructed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ouew3fwYMc5z"},"outputs":[],"source":["# Identificar colunas não numéricas\n","non_numeric_cols = X.select_dtypes(include=['object']).columns\n","\n","# Aplicar Label Encoding ou One-Hot Encoding nas colunas categóricas\n","for col in non_numeric_cols:\n","    le = LabelEncoder()\n","    X[col] = le.fit_transform(X[col])\n","\n","# Verifique o resultado\n","print(X.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tmMwEeGAaxfX"},"outputs":[],"source":["numerical_cols = X.select_dtypes(include=['number']).columns  # Get numerical columns from X\n","scaler = MinMaxScaler()  # Create a new or reset the existing scaler\n","X[numerical_cols] = scaler.fit_transform(X[numerical_cols]) # Fit and transform on X"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U0VEXUCqqeXW"},"outputs":[],"source":["# Fazer previsões (reconstrução das entradas)\n","# Após o treinamento, o autoencoder tenta reconstruir as amostras de entrada\n","reconstructed = autoencoder.predict(X)\n","print(\"Dados originais:\")\n","print(X_sample)\n","print(\"\\nDados reconstruídos:\")\n","print(reconstructed)  # Exibe os dados reconstruídos, que devem ser próximos aos dados originais"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tdo9bbPHVRg3"},"outputs":[],"source":["# import pandas as pd\n","# from sklearn.preprocessing import LabelEncoder\n","\n","# # Assuming X is a pandas DataFrame\n","# # ... your existing code for creating the autoencoder model ...\n","\n","# # 1. Identify string columns in X\n","# string_cols = X.select_dtypes(include=['object']).columns\n","\n","# # 2. Create a LabelEncoder for each string column\n","# label_encoders = {}\n","# for col in string_cols:\n","#     label_encoders[col] = LabelEncoder()\n","#     X[col] = label_encoders[col].fit_transform(X[col])\n","\n","# # 3. Now you can train the model\n","# autoencoder.fit(X, X, epochs=100, verbose=0)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}